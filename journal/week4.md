# Week 4 â€” Postgres and RDS

## Create RDS Postgres Instance

We started Week 4 learning about PostgreSQL databases. We made sure all bootcampers had PostGres installed, made sure it was added to gitpod.yml, and started onward. 

Once we got under way, we created the RDS Postgres instance from the AWS CLI in Gitpod. Andrew had specified the commands needed to create this, but we slightly modified the existing. I missed the part where we updated the username for the database, which later caused me a slight hiccup in following along. Don't worry about the password being passed here, I updated it later. 

```
aws rds create-db-instance \
  --db-instance-identifier cruddur-db-instance \
  --db-instance-class db.t3.micro \
  --engine postgres \
  --engine-version  14.6 \
  --master-username root \
  --master-user-password Dbpassword1 \
  --allocated-storage 20 \
  --availability-zone us-east-1a \
  --backup-retention-period 0 \
  --port 5432 \
  --no-multi-az \
  --db-name cruddur \
  --storage-type gp2 \
  --publicly-accessible \
  --storage-encrypted \
  --enable-performance-insights \
  --performance-insights-retention-period 7 \
  --no-deletion-protection
```
After this ran, we connected to AWS and looked at RDS. From here, we could see our database. 

![RDSDBWeek4](https://user-images.githubusercontent.com/119984652/226061414-5da31363-2959-4c17-b819-aa410bd55987.png)

We commented out our code for DynamoDB in docker-compose.yml, as Andrew said he didn't want us running an extra container for it currently, since plenty of bootcampers are running out of Gitpod credits frequently.

Andrew then showed us how to connect to Postgres locally, using this command:

```
psql -Upostgres --host localhost
```

After entering our credentials to access the account, we then walked through several PSQL commands.

```sql
\x on -- expanded display when looking at data
\q -- Quit PSQL
\l -- List all databases
\c database_name -- Connect to a specific database
\dt -- List all tables in the current database
\d table_name -- Describe a specific table
\du -- List all users and their roles
\dn -- List all schemas in the current database
CREATE DATABASE database_name; -- Create a new database
DROP DATABASE database_name; -- Delete a database
CREATE TABLE table_name (column1 datatype1, column2 datatype2, ...); -- Create a new table
DROP TABLE table_name; -- Delete a table
SELECT column1, column2, ... FROM table_name WHERE condition; -- Select data from a table
INSERT INTO table_name (column1, column2, ...) VALUES (value1, value2, ...); -- Insert data into a table
UPDATE table_name SET column1 = value1, column2 = value2, ... WHERE condition; -- Update data in a table
DELETE FROM table_name WHERE condition; -- Delete data from a table
```

Once we were logged into the local database, we created a database named cruddur. 

```sql
CREATE database cruddur;
```

We now needed to setup tables for the database. Andrew explained in other languages like Ruby on Rails, they have a schema that defines the entire database. Since Flask doesn't have it, we are going to write the entire thing by hand instead. To begin this, we created a new folder in 'backend-flask' named 'db' then created a new SQL file named schema.sql inside of that. As part of schema.sql, we need to have Postgres generate UUIDs. These are universally unique identifiers. It's a randomly generated string of numbers generated by Postgres. To implement this, we needed to add an extension for it. 

```sql
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

```
After this, from our postgres:bash terminal, we ran: 

```
psql cruddur < db/schema.sql -h localhost -U postgres
```
This created the extension needed. To make doing this easier, we created a connection url string for our local database.

```
export CONNECTION_URL="postgresql://postgres:thisisntmypassword@localhost:5432/cruddur"
gp env export CONNECTION_URL="postgresql://postgres:thisisntmypassword@localhost:5432/cruddur"
```

We then created a connection url string for our prod RDS database as well, then set the environment variable there too.

```
export PROD_CONNECTION_URL="postgresql://root:thisisntmyproductionpassword@cruddur-db-instance.c8ersdfsdf.us-east-1.rds.amazonaws.com:5432/cruddur"
gp env PROD_CONNECTION_URL="postgresql://root:thisisntmyproductionpassword@cruddur-db-instance.c8ersdfsdf.us-east-1.rds.amazonaws.com:5432/cruddur"
```
The format for a connection url string for a Postgres database is the following: 

```
postgresql://[user[:password]@][netlocation][:port][/dbname][?parameter1=value1]
```
## Bash scripting for common database actions

We then created a new folder in 'backend-flask' named 'bin' which stands for binary. In this folder, we can store batch scripts to execute commands for our database. We then made several new files: db-create, db-drop, db-schema-load

![image](https://user-images.githubusercontent.com/119984652/226064608-d9d7315a-0346-4ebc-8864-65c5be25ef13.png)

For the files, each one needs a shebang. The shebang tells our app how to treat this file. Since we want it to run as bash, we added:

```
#! /user/bin/bash
```
We started off testing if we can drop our database. So to do this, we add a line to db-drop.

```
psql $CONNECTION_URL -c "drop database cruddur;'
```

This command should psql connect to our local Postgres database using our connection string url, then using '-c' it issues a SQL command of "drop database cruddur;"

We run db-drop from our terminal and it gives us permission denied. This is because the file is not executable by default. To make it or any file executable after creation, we have to run an additional command for it:

```
chmod u+x <filename>
```

We run this to make our file executable, then do the same for our remaining batch scripts. We again test our batch script and get an error letting us know that we cannot drop the currently open database. Since we're going to be working with this frequently, we need to replace part of our CONNECTION_URL, so we're not using the database when trying to drop it. To do this, we must use sed. Sed is a text stream editor used on Unix systems to edit files quickly and efficiently. The tool searches through, replaces, adds, and deletes lines in a text file without opening the file in a text editor. We use sed, then wrap it in an environment variable.

```
#! /usr/bin/bash

echo "db-drop"

NO_DB_CONNECTION_URL=$(sed 's/\/cruddur//g' <<<"$CONNECTION_URL")
psql $NO_DB_CONNECTION_URL -c "DROP database cruddur;"
```

We then go through and do the same for our db-create, passing a different SQL command.

```
#! /usr/bin/bash

echo "db-create"

NO_DB_CONNECTION_URL=$(sed 's/\/cruddur//g' <<<"$CONNECTION_URL")
psql $NO_DB_CONNECTION_URL -c "CREATE database cruddur;"
```

We move onto db-schema-load, and add a different command to connect to our database. Since it runs relative to where we're executing, we use realpath. The realpath will tell us the actual path of the file. 

```
#! /usr/bin/bash

echo "db-schema-load"

schema_path="$(realpath .)/db/schema.sql"
echo $schema_path

psql $CONNECTION_URL cruddur < $schema_path
```

We need a way to determine when we're running from our production environment (prod) or our local Postgres environment. To do this, we added an if statement to the code.

```
#! /usr/bin/bash

CYAN='\033[1;36m'
NO_COLOR='\033[0m'
LABEL="db-schema-load"
printf "${CYAN}== ${LABEL}${NO_COLOR}\n"

schema_path="$(realpath .)/db/schema.sql"
echo $schema_path

if [ "$1" = "prod" ]; then
  echo "Running in production mode"
  URL=$PROD_CONNECTION_URL
else
  URL=$CONNECTION_URL
fi

psql $URL cruddur < $schema_path
```

The additional code under our shebang was also added to provide a different color through the CLI when viewing our Postgres logs, so we can see when this is being ran. We went through and added it to our other batch scripts as well. 

Back on our main task of adding tables to our database, we go back to our schema.sql, and add SQL commands to create our tables:

```sql
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
DROP TABLE IF EXISTS public.users;
DROP TABLE IF EXISTS public.activities;


CREATE TABLE public.users (
  uuid UUID DEFAULT uuid_generate_v4() PRIMARY KEY,
  display_name text,
  handle text,
  cognito_user_id text,
  created_at TIMESTAMP default current_timestamp NOT NULL
);

CREATE TABLE public.activities (
  uuid UUID DEFAULT uuid_generate_v4() PRIMARY KEY,
  user_uuid UUID NOT NULL,
  message text NOT NULL,
  replies_count integer DEFAULT 0,
  reposts_count integer DEFAULT 0,
  likes_count integer DEFAULT 0,
  reply_to_activity_uuid integer,
  expires_at TIMESTAMP,
  created_at TIMESTAMP default current_timestamp NOT NULL
);
```

We then made a new batch script named 'db-connect', made it executable by running 'chmod u+x ./bin/db-connect' then ran the file we just created.

```
#! /usr/bin/bash

psql $CONNECTION_URL
```

We were able to successfully connect to our local Postgres database using this batch script. We then ran '\dt' from the Postgres database to view our tables.

![image](https://user-images.githubusercontent.com/119984652/226068393-950132a1-5b3a-4ff7-b8ec-70b8429ff510.png)

We then created one more batch script named 'db-seed' and made it executable as well.

```
#! /usr/bin/bash

CYAN='\033[1;36m'
NO_COLOR='\033[0m'
LABEL="db-seed"
printf "${CYAN}== ${LABEL}${NO_COLOR}\n"

seed_path="$(realpath .)/db/seed.sql"
echo $seed_path

if [ "$1" = "prod" ]; then
  echo "Running in production mode"
  URL=$PROD_CONNECTION_URL
else
  URL=$CONNECTION_URL
fi

psql $URL cruddur < $seed_path
```

This batch script will run our seed.sql file that we created in 'backend-flask/db'. 

```sql
-- this file was manually created
INSERT INTO public.users (display_name, handle, cognito_user_id)
VALUES
  ('Andrew Brown', 'andrewbrown' ,'MOCK'),
  ('Andrew Bayko', 'bayko' ,'MOCK');

INSERT INTO public.activities (user_uuid, message, expires_at)
VALUES
  (
    (SELECT uuid from public.users WHERE users.handle = 'andrewbrown' LIMIT 1),
    'This was imported as seed data!',
    current_timestamp + interval '10 day'
  )
```

When we test the 'db-seed' file, it works.

![image](https://user-images.githubusercontent.com/119984652/226068916-4b9bcbc3-706a-4214-87f2-06cd45f668b6.png)

## Install Postgres Driver in Backend Application

We may need to see what connections are being used to our Postgres database. For this, we implement 'db-sessions' and make it executable.

```
#! /usr/bin/bash
CYAN='\033[1;36m'
NO_COLOR='\033[0m'
LABEL="db-sessions"
printf "${CYAN}== ${LABEL}${NO_COLOR}\n"

if [ "$1" = "prod" ]; then
  echo "Running in production mode"
  URL=$PROD_CONNECTION_URL
else
  URL=$CONNECTION_URL
fi

NO_DB_URL=$(sed 's/\/cruddur//g' <<<"$URL")
psql $NO_DB_URL -c "select pid as process_id, \
       usename as user,  \
       datname as db, \
       client_addr, \
       application_name as app,\
       state \
from pg_stat_activity;"
```

When we run this, we're able to see the active connections to our database, and close whatever connections we need to close. 

![image](https://user-images.githubusercontent.com/119984652/226069525-5c860edd-8503-4319-b9d5-d8c7ea0e2e09.png)

We then drop our database by running 'db-drop' but decide we should create a command to run all of our commands, so we don't have to run them invidivually. We create 'db-setup' and make it executable.

```
#! /usr/bin/bash
-e # stop if it fails at any point

CYAN='\033[1;36m'
NO_COLOR='\033[0m'
LABEL="db-setup"
printf "${CYAN}==== ${LABEL}${NO_COLOR}\n"

bin_path="$(realpath .)/bin"

source "$bin_path/db-drop"
source "$bin_path/db-create"
source "$bin_path/db-schema-load"
source "$bin_path/db-seed"
```
With that setup, now we could now install the Postgres driver in our Backend application. We're going to use an AWS Lambda to to insert users into our database. Since AWS lacks the required PostgreSQL libraries in the AMI image, we must run the Postgres driver, custom compliled Psycopg2 C library for Python. To implement this, we add to our requirements.txt file. 

```
psycopg[binary]
psycopg[pool]
```

We then do a 'pip install -r requirements.txt' to install the driver. 

## Connect Gitpod to RDS Instance

We're going to have to use pooling as a way of dealing with connections to our database. There's a certain amount of connections your database can handle in relational databases. There's concurrent connections, where some are running, some aren't. These connection pools allow us to re-use connections with concurrent connections. Since we're running Lambda functions, if our app became widely popular, we'd need to use a proxy, as Lambda functions create new functions each time it's ran, which could become expensive.

To create our connection pool, we create a new file in 'backend-flask/lib' named 'db.py'

```py
from psycopg_pool import ConnectionPool
import os

def query_wrap_object(template):
  sql = f"""
  (SELECT COALESCE(row_to_json(object_row),'{{}}'::json) FROM (
  {template}
  ) object_row);
  """
  return sql

def query_wrap_array(template):
  sql = f"""
  (SELECT COALESCE(array_to_json(array_agg(row_to_json(array_row))),'[]'::json) FROM (
  {template}
  ) array_row);  
  """   
  return sql

connection_url = os.getenv("CONNECTION_URL")  
pool = ConnectionPool(connection_url)
```

We open docker-compose.yml and add an environment variable for our CONNECTION_URL.

```yml
CONNECTION_URL: "postgresql://postgres:password@localhost:5432/cruddur"
```

Next we open home_activities.py to import our connection pool, remove our mock data, and add our query to establish our connection. 

```py
from lib.db import pool, query_wrap_array

...........................

    sql = query_wrap_array("""
    SELECT
        activities.uuid,
        users.display_name,
        users.handle,
        activities.message,
        activities.replies_count,
        activities.reposts_count,
        activities.likes_count,
        activities.reply_to_activity_uuid,
        activities.expires_at,
        activities.created_at
      FROM public.activities
      LEFT JOIN public.users ON users.uuid = activities.user_uuid
      ORDER BY activities.created_at DESC
    """)
    print("SQL=========")    
    print(sql)
    print("SQL+++++++++")       
    with pool.connection() as conn:
      with conn.cursor() as cur:
        cur.execute(sql)
        # this will return a tuple
        # the first field being the data
        json = cur.fetchone()
    print("---------") 
    print(json[0])      
    return json[0]   
    return results

```
This will fetch the data and return the results. Since we're writing raw SQL, this will allows us to return json directly as well. 

After working through some SQL errors, we pointed our attention back towards RDS. We spin up our RDS database, then test connecting to it using the terminal.

```
psql $PROD_CONNECTION_URL
```

Since we have not setup access through the security group for our RDS yet, the connection hangs. We must get the IP address of our Gitpod environment, then give that to our security group in AWS. We manually setup an inbound rule in our security group through AWS. To do this, from the terminal, we run 'curl ifonfig.me' which outputs our Gitpod IP address. We next passed GITPOD_IP=$(curl ifconfig.me) as variable so we can grab GITPOD_IP for RDS whenever needed. This allowed us to store our current IP address as an environment variable.  

We again test the 'psql' command above, this time it works. Since our IP is going to update everytime we launch our workspace, we will need to manually update that IP stored by the inbound rule everytime as well.

There's several env variables we then set after this, passing our security group id and our security group rule id as variables: DB_SG_ID and DB_SG_RULE_ID

We also store our env var in '.gitpod.yml' as well as create a new batch script named 'rds-update-sg-rule' to run every time our environment launches:

```yml
  - name: postgres
    init: |
      curl -fsSL https://www.postgresql.org/media/keys/ACCC4CF8.asc|sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/postgresql.gpg
      echo "deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main" |sudo tee  /etc/apt/sources.list.d/pgdg.list
      sudo apt update
      sudo apt install -y postgresql-client-13 libpq-dev      
      sudo apt install -y postgresql-client-13 libpq-dev  
    command: |
      export GITPOD_IP=$(curl ifconfig.me)
      source  "$THEIA_WORKSPACE_ROOT/backend-flask/bin/rds-update-sg-rule" 
```

```
#! /usr/bin/bash

CYAN='\033[1;36m'
NO_COLOR='\033[0m'
LABEL="rds-update-sg-rule"
printf "${CYAN}== ${LABEL}${NO_COLOR}\n"

aws ec2 modify-security-group-rules \
    --group-id $DB_SG_ID \
    --security-group-rules "SecurityGroupRuleId=$DB_SG_RULE_ID,SecurityGroupRule={Description=GITPOD,IpProtocol=tcp,FromPort=5432,ToPort=5432,CidrIpv4=$GITPOD_IP/32}"
```

After confirming connection to RDS from Gitpod, modified docker-compose.yml to pass a different env var for CONNECTION_URL. 

```yml
CONNECTION_URL: "${PROD_CONNECTION_URL}"
```

The new variable didn't work because we did not have a schema loaded. I then accessed backend-flask, then ran './bin/db-schema-load prod'. This got rid of the 'psycopg.errors.UndefinedTable: relation "public.activities" does not exist error' we were receiving. 

## Create Congito Trigger to insert user into database and Create new activities with a database insert

When we load the webpage, it's still blank, but this is because there's no data. To create data, we need a user to sign up. When we sign up through Cognito, we need to create a record, which can be done through a Lambda. 

We setup the Lambda function through AWS, and needed to setup an environment variable for it. We created CONNECTION_URL, which was the PROD_CONNECTION_URL we set earlier through AWS CLI for Gitpod. Next we needed to add a layer for the function to interact with the Psycopg2 library, so we referenced https://github.com/jetbridge/psycopg2-lambda-layer where I found py 3.8 for my region (us-east-1), then inserted the ARN of the function into the console and verified: it's compatible, so it was added to the function. 

Added Lambda function to our Cognito User pool through Cognito > User pools, then selecting our user pool. It's a trigger type: sign-up, post confirmation trigger. I then selected the function we already configured as the function to use.

Tested Lambda function by opening Cloudwatch logs, then going to Cognito and removing our existing user we created a couple weeks ago. We then attempted to recreate our user account through our site, but received a "PostConfirmation failed with error local variable 'conn' referenced before assignment." error. Viewed Cloudwatch log for this error through the AWS console.

![CloudwatchLogEventsWeek4](https://user-images.githubusercontent.com/119984652/226073900-3fb9ed3b-2585-49a0-9301-e411d6c99503.png)

The error indicates a problem with our code in the cruddur-post-confirmation.py we created earlier. Updated indentation to the Lambda function code, then copied new code to Lambda function and deployed the changes. Tried web site again, resending a new authentication code. This time, we received a new error.

![NewCruddurErrorWeek4](https://user-images.githubusercontent.com/119984652/226073923-3b4b4c38-cfe2-44d8-a094-9ef7cabc1329.png)

This could possibly be due to the website trying to reach the Postgres DB, but is unable to. We got back to Cloudwatch and view the new logs.

![NewCloudWatchLogsWeek4](https://user-images.githubusercontent.com/119984652/226073955-99bd4cbf-75d9-46bd-994c-3c2270abdc00.png)

We updated our Lambda function again, this time removing conn = psycopg2.connect(os.getenv('CONNECTION_URL')) and cur = conn.cursor() from 'try' and adding print commands to pass a string of "SQL Statement ---" and sql itself. We again went back to Cognito and removed our user, then signed up through the web site again. We again got a timeout error, but we knew this would happen. We just wanted to see what information is passing. 

After some corrections to syntax, we went back to Lambda and found that we needed to set a VPC for our function. To do this, we needed to set a permission for the lambda. We went to the Lambda Configuration, then selected Permissions, and selected the execution role. This redirected us to the IAM page under Roles for the function's execution role. Add permissions/Attach policies > then found we needed to create a custom policy to allow Lambda to create a network interface to the db. We then attached the policy to our Lambda function through IAM to add the permissions. 

We again went back and created a VPC for our function, this time choosing our VPC, 2 subnets for better availability, and our default VPC security group. 

![LambdaFunctionVPCWeek4](https://user-images.githubusercontent.com/119984652/226074037-584b5fe9-4d60-40e3-9d8b-4d868cc4ab3f.png)

Back in Cognito, we again removed our user, then went back to our web app to test again. We again got the same error from our website. I find that Andrew did not, so I repeat the same steps: removed user from Cognito, then went back to our web app and created an account again. This time, I get the same response as Andrew. In the latest Cloudwatch log, we were now getting a syntax error.

![SyntaxErrorWeek4](https://user-images.githubusercontent.com/119984652/226074090-2ce850e9-2824-4c8d-8ab8-9504cd1f3d6f.png)

We reviewed our function once again, and after clearing up a few more syntax errors with our SQL command, we again tested. I came back with a successful new user created, but Andrew during the video got an error in regards to the email because his schema was not loaded. This led us into looking at our schema.sql file and editing the tables for public.users to not include NULL data, and we also reran the cmd to load the schema from the AWS CLI. "./bin/db-schema-load prod"

Once more we tested our function by removing the user from Cognito, creating a new account through our web app, then again checking our Cloudwatch logs. This time, the user was successfully created without errors.

![CloudWatchLogEventNoErrorsWeek4](https://user-images.githubusercontent.com/119984652/226074143-b6ada97f-730c-4b48-a184-10b071bd1038.png)

To test this from the terminal, we selected our Postgres bash terminal in Gitpod, then connected to our production database: ./bin/db-connect prod. Once connected, we queried the users table: select * from users; > this returns 1 row, our newly created user account! 

![SQLQueryforAllFromUsersWeek4](https://user-images.githubusercontent.com/119984652/226074170-780fb110-d76c-4562-ad1f-35dbffe5f42b.png)

Next, we spun up our environment and login and find there's no data. This is because we need to create it. We begin by creating activities. While working out the code for actvities, we found a function to add to db.py from https://kb.objectrocket.com/postgresql/python-error-handling-with-the-psycopg2-postgresql-adapter-645. Defined a new function in db.py to clean up code in create_activity.py. Called function in home_activities.py instead. Wrapped all functions in db.py by defining new class Db and importing to home_activities.py. 

We then created SQL templates. We pasted query from create_activity.py to new create.sql. We worked through various errors in the connection pool, and added color to the errors to see what's running and what isn't. At this point, I must've missed something in the queries, as Andrew was able to post a Crud by this point, while I could not.  

I went over each video this week again and again trying to find what I missed. I began troubleshooting the error I was receiving, as I continued over and over when trying to post a Crud the error of 'sycopg.errors.NotNullViolation'. I even created another user in my user pool with Andrew's user_handle(preferred_username since that's what the Lambda is passing) and swapped the hardcoded user_handle in app.py back to 'andrewbrown' and it made no difference.

![userhandlehardcodeWeek4](https://user-images.githubusercontent.com/119984652/226074676-9258c5eb-2fc6-4fc2-a237-0c8a86475e68.png)

If I ran db-connect from prod, then run SELECT * FROM users or activities, both tables come back empty.

![EmptyQueryWeek4](https://user-images.githubusercontent.com/119984652/226074707-e306e39b-bfc4-4b69-8ced-b625241eb878.png)

After asking Andrew about this through Discord, he gave me a lot of good information.

![AndrewDiscordWeek4](https://user-images.githubusercontent.com/119984652/226074730-ffbf5377-3811-4aa9-a06e-87d7ee8c1bc4.png)
![AndrewDiscordWeek4contd](https://user-images.githubusercontent.com/119984652/226074739-592f7274-0bd8-4995-b0ed-30a30e0a167b.png)

I began troubleshooting with the information Andrew gave me. 

Andrew: That Lambda is setup to the user production database. So where you could be running into an issue is that the user is inserted into the production RDS postgres database

ME: when I tried to run a SELECT * query while in psql from local postgres or RDS, both activities and users were empty, even though earlier I was able to seed the mock data in a previous video

Andrew: And then you could be using your local database which has no users inserted, so you have to manually create them

ME:  whenever I would try to run the db-setup command either locally or from prod, I'd get a not-null violation

![NullValuesWeek4](https://user-images.githubusercontent.com/119984652/226074790-8ce42b83-a474-4cea-b2c1-72b5fd7a700b.png)

Andrew: I updated the seed data locally to create the mock users and copy the cognito_user_id and hardcode it into the seed data

ME: I could insert mock data into the page while following along with the video for seed.sql, but felt I must've missed a step on this portion

Andrew: determine which db your flask is running, local postgres or production RDS postgres

ME: I was able to verify the correct db was running by running "echo $CONNECTION_URL" from the shell of the backend, it returned my RDS postgres login

Andrew: determine why the seed data is not inserting the user

ME: I had no idea on this part. 

Andrew: determine if you have have the cognito_user_id hardcoded into the seed.sql file

ME: I viewed my seed.sql file, and cognito_user_id is listed, so I was good here.

Andrew: determine that the query works by testing the query manually in psql or the database explorer for the commit command

ME: I tried to do this through psql to both PROD_CONNECTION_URL and CONNECTION_URL using the queries from seed.sql, and both failed:

![failedqueriesWeek4](https://user-images.githubusercontent.com/119984652/226074914-1d542005-e08c-498d-af52-c0626d437a7e.png)

The next morning, I checked Discord and found that another Bootcamper, OG10#7292 had responded to the thread I had been speaking with Andrew on.

![OG10response](https://user-images.githubusercontent.com/119984652/226074929-ff93a3e4-0c9a-47b7-8dec-d338f5382feb.png)

Under their suggestion, I added an email for both mock data users, then ran db-setup (since it runs all of our commands to create, load the schema, seed the data, etc) and still got the same error, but it gave me a bit more information. I realized my seed.sql INSERT was missing a column for email, and added it.

After that, I ran db-setup from both local (just ./bin/db-setup) and prod(./bin/db-setup prod) to create my database, create the extension, create the tables, seed the data, etc.  since I was getting no data when I'd psql to the database and check the tables (SELECT * FROM users;). When this came back with "INSERT" I was pretty sure I was on the right track.

I next logged into Cruddur, and posted a Crud which posted!!!! I refreshed the page, and just as Andrew's did in the video, it joined the databases, and posted the "This was imported as seed data!" Now the only issue was Cruds post as Andrew, and not as myself. I found several other Bootcampers fixed this by changing the hardcoded value for 'user_handle' from 'andrewbrown' to their own preferred username, and it fixed it. I also found another thread on Discord from anle4s#7774 where they passed the user_handle prop in the ActivityForm component, updated the fetch request body of the ActivityForm.js, and reassigned the user_handle variable in app.py as well. These changes will ensure that the user_handle prop is passed correctly and included in the fetch request, and that the server can retrieve it from the request payload. 
